{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Q&A Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "API_OPENAI = os.getenv(\"API_OAI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadint PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os \n",
    "    _, extension = os.path.splitext(file)\n",
    "    if extension == \".pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader # import the `pypdf` module to make it work\n",
    "        print(f\"Loading {file}...\")\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == \".docx\":\n",
    "        from langchain.document_loaders import Docx2txtLoader # import the `docx2txt` module to make it work\n",
    "        print(f\"Loading {file}...\")\n",
    "        loader = Docx2txtLoader(file)\n",
    "    data = loader.load()\n",
    "    print(\"Done ... \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading us_constitution.pdf...\n",
      "Done ... \n",
      "You have 41 pages in you data\n"
     ]
    }
   ],
   "source": [
    "data = load_document(file=\"us_constitution.pdf\")\n",
    "print(f\"You have {len(data)} pages in you data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the_great_gatsby.docx...\n",
      "Done ... \n",
      "You have 1 pages in you data\n"
     ]
    }
   ],
   "source": [
    "data = load_document(file=\"the_great_gatsby.docx\")\n",
    "print(f\"You have {len(data)} pages in you data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External source, e.g., Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wikipedia -q\n",
    "def load_from_wikipedia(query, lang=\"en\", load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model trained and created by OpenAI and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI's API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 \n",
      "Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has not revealed technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "\n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\", which was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with over 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,048 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.\n",
      "To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.\n",
      "When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.\n",
      "A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existin\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia(\"GPT4\")\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size = 256):\n",
    "\t# for generic text, the one below is the recommended text splitter\n",
    "\t# by default: \\n \\\\n and white space\n",
    "\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\ttext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "\tchunks = text_splitter.split_documents(data)\n",
    "\treturn chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading us_constitution.pdf...\n",
      "Done ... \n",
      "You have 41 pages in you data\n"
     ]
    }
   ],
   "source": [
    "data = load_document(file=\"us_constitution.pdf\")\n",
    "print(f\"You have {len(data)} pages in you data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data=data)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Uploading to a Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_PINECONE = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index if does not exists, embedd the chunks and insert into the index. If exists, we load from the index.\n",
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "\timport pinecone\n",
    "\tfrom langchain_community.vectorstores import Pinecone\n",
    "\tfrom langchain_openai import OpenAIEmbeddings\n",
    "\tfrom pinecone import ServerlessSpec\n",
    "\t\n",
    "\tpc = pinecone.Pinecone(api_key=API_PINECONE)\n",
    "\tembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1536, api_key = API_OPENAI)\n",
    "\t\n",
    "\tif index_name in pc.list_indexes().names():\n",
    "\t\tprint(f\"{index_name} already exists. Loading embeddings ... \", end=\"\")\n",
    "\t\tvector_stores = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\t\tprint(\"ok\")\n",
    "\telse:\n",
    "\t\tprint(f\"Creating index {index_name} and embeddings ... \", end=\"\")\n",
    "\t\tpc.create_index(\n",
    "\t\t\tname = index_name,\n",
    "\t\t\tdimension=1536,\n",
    "\t\t\tmetric = \"cosine\",\n",
    "\t\t\tspec= ServerlessSpec(\n",
    "\t        cloud=\"aws\",\n",
    "\t        region=\"us-east-1\"\n",
    "\t    \t) \n",
    "\t\t)\n",
    "\t\tvector_stores = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "\t\t\n",
    "\t\tprint(\"ok\")\n",
    "\t\treturn vector_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete index in the free tier of Pinecone (only one available)\n",
    "def delete_pinecone_index(index_name = \"all\"):\n",
    "\timport pinecone\n",
    "\tpc = pinecone.Pinecone(api_key = API_PINECONE)\n",
    "\tif index_name == \"all\":\n",
    "\t\tindexes = pc.list_indexes().names()\n",
    "\t\tprint(\"Deleting all indexes....\")\n",
    "\t\tfor index in indexes:\n",
    "\t\t\tpc.delete_index(index)\n",
    "\t\tprint(\"Ok\")\n",
    "\telse:\n",
    "\t\tprint(f\"Deleting index {index_name}.....\", end=\"\")\n",
    "\t\tpc.delete_index(index_name)\n",
    "\t\tprint(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximecollet/Desktop/Bureau - iMac de Maxime/DOC MAX/Courses/ZTM_LangChain_LLM/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes....\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index ask-a-document and embeddings ... ok\n"
     ]
    }
   ],
   "source": [
    "vector_store = insert_or_fetch_embeddings(index_name=\"ask-a-document\", chunks=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(api_key=API_OPENAI, model=\"gpt-4\", temperature=1)\n",
    "    \n",
    "    # Use the as_retriever() method to get the retriever from the vector store\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "    \n",
    "    # Create the RetrievalQA chain\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    # Get the answer\n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"What is the whole document about ?\"\n",
    "answer = ask_and_get_answer(vector_store, q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but there is no document provided for me to analyze. Could you please provide the document or information you're referring to?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n",
      "\n",
      "Answer: I'm sorry, but I can't assist you unless you provide a question or any context. How can I assist you today?\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "\n",
      "Answer: The document is the United States Constitution. It outlines the principles, structures, and processes of the U.S. government, including the purposes of forming a union, establishing justice, ensuring domestic tranquility, providing for common defense, and promoting general welfare. It also mentions that the Constitution, U.S. laws, and treaties are the supreme law of the country.\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "Quitting ... bye bye \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print(\"Write Quit or Exit to quit.\")\n",
    "\n",
    "while True:\n",
    "\tq = input(f\"Question #{i}\")\n",
    "\ti += 1\n",
    "\tif q.lower() in [\"quit\",\"exit\"]:\n",
    "\t\tprint(\"Quitting ... bye bye \")\n",
    "\t\ttime.sleep(2)\n",
    "\t\tbreak\n",
    "\tanswer = ask_and_get_answer(vector_store, q)\n",
    "\tprint(f\"\\nAnswer: {answer[\"result\"]}\")\n",
    "\tprint(f\"\\n {\"-\"*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
